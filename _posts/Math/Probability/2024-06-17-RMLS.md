---
title: æ­£å‰‡åŒ–ãƒŸãƒ‹ãƒãƒ ãƒãƒ«ãƒ è§£(Regularized minimal-norm solution)
date: 2024-06-17 11:03 +0900
categories: [Math, Probability Theory]
tags: [Probability, Linear Regression, Regularized Minimal-Norm Solution, Math, JP]
author: Youkoutaku
math: true
img_path: /src/math/msB1/
image:
  path: output_26_1.png
  alt: 
---

Home Page:[Youkoutaku](https://youkoutaku.github.io/)

```python
import numpy as np
import scipy as sp
import pandas as pd
from pandas import Series, DataFrame
from sklearn.metrics import mean_squared_error

import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
%matplotlib inline

# å°æ•°ç¬¬3ä½ã¾ã§è¡¨ç¤º
%precision 3

# ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã®å›ºå®š
np.random.seed(123)

```

## æ­£å‰‡åŒ–ãƒŸãƒ‹ãƒãƒ ãƒãƒ«ãƒ è§£é–¢æ•°


```python
# æ­£å‰‡åŒ–ãƒŸãƒ‹ãƒãƒ ãƒãƒ«ãƒ è§£ã‚’æ±‚ã‚ã‚‹ rmls(å­¦ç¿’ãƒ‡ãƒ¼ã‚¿x(x_train), å­¦ç¿’ãƒ‡ãƒ¼ã‚¿y(y_train), è§£ç©ºé–“ã®æ¬¡å…ƒæ•°N, æ­£è¦åŒ–å®šæ•°Î¾)
def lms(x_train, y_train, N, gzai ):

    #è¡Œåˆ—Hã®è¡Œæ•°è¨­å®š
    M=x_train.shape[0]

    #xã¨yã‚’Mè¡Œ1åˆ—ã«å¤‰æ›
    x_train=x_train.reshape((M,1))
    y_train=y_train.reshape((M,1))

    #å…¨ã¦ã®è¦ç´ ãŒ1ã®åˆ—ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ
    i=np.ones((M,1))

    H=i
    for k in range(1,N):
        H=np.hstack((H,x_train**k))
    #print('H =', H)


    #ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Î˜ã®æœ€å°äºŒä¹—æ¨å®šå€¤
    A=np.dot(H, H.T)+gzai*np.eye(M)

    lss_c=np.dot(np.dot(H.T,np.linalg.inv(A)),y_train)

    #åˆ†è§£èƒ½è¡Œåˆ—Qã®è¨ˆç®—
    Q=np.dot(np.dot(H.T,np.linalg.inv(A)),H)

    #lss_cã®è¦ç´ ã‚’é€†é †ã«ä¸¦ã³æ›¿ãˆ
    lss_c=np.sort(lss_c)[::-1]

    return (lss_c, M, Q)
```

## é–¢æ•°1

### è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
ã‚µãƒ³ãƒ—ãƒ«ï¼š$20$ï¼ˆå­¦ç¿’ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†ã‘ã‚‹ã“ã¨ã§ï¼Œè¦³æ¸¬ç©ºé–“M=10ï¼‰

å…ƒã®é–¢æ•°ï¼š

$$y=sin(x)+cos(x), 0<x<2$$

ãƒã‚¤ã‚ºï¼š
å¹³å‡0,æ¨™æº–åå·®0.3ã®æ­£è¦åˆ†å¸ƒ



```python
x = []
y = []
#ã‚µãƒ³ãƒ—ãƒ«æ•°
sample = 20
#ãƒã‚¤ã‚ºæ¨™æº–åå·®
sigma=0.3

# ãƒã‚¤ã‚ºï¼ˆæ­£è¦åˆ†å¸ƒï¼‰
# np.random.normal(å¹³å‡ã€æ¨™æº–åå·®ã€ã‚µãƒ³ãƒ—ãƒ«æ•°)
e= np.random.normal(0, sigma, sample)
#eo= np.random.normal(0, sigma, m)


# ä¿¡å·ç”Ÿæˆ
#np.random.uniform(a, b, ã‚µãƒ³ãƒ—ãƒ«æ•°) aã‹ã‚‰ï½‚ã¾ã§ã®ä¸€æ§˜ä¹±æ•°
x=np.random.uniform(0, 2, sample)
y = np.sin(np.pi*x) - np.cos(np.pi*x)  + e

# ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ç”¨ã¨æ¤œè¨¼ç”¨ã«åˆ†å‰²
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.5, test_size = 0.5, random_state = 0)
```


```python
plt.scatter(x_train, y_train)
plt.xlabel('x_train')
plt.ylabel('y_train')
plt.grid(True)
```


    
![png](output_7_0.png)
    



```python
plt.scatter(x_test, y_test)
plt.xlabel('x_test')
plt.ylabel('y_test')
plt.grid(True)
```


    
![png](output_8_0.png)
    


### è§£ç©ºé–“Nã®è€ƒå¯Ÿ


```python
diff = []
result= []
for N in range(2,30):
  lss_c, M, Q = lms(x_train, y_train, N, 0.1)
 #å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦yã‚’æ¨å®š
  ys_train= np.polyval(lss_c,x_train)
 #æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦yã‚’æ¨å®š
  ys_test=np.polyval(lss_c,x_test)
 #å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹yã®æ¨å®šå€¤ã®å¹³å‡äºŒä¹—èª¤å·®
  train_error= mean_squared_error(y_train, ys_train)
 #æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹yã®æ¨å®šå€¤ã®å¹³å‡äºŒä¹—èª¤å·®
  test_error= mean_squared_error(y_test, ys_test)
  result.append([train_error, test_error])

diff = DataFrame(data=result, columns=['Training','Test'], index=[n for n in range(2,30)])

diff.plot(title='Mean Squared Error')
plt.xlabel('N')
plt.ylabel('MSE')
plt.grid(True)
```


    
![png](output_10_0.png)
    


ã“ã‚Œã«ã‚ˆã‚Šï¼Œè§£ç©ºé–“NãŒå¤§ãããªã‚‹ã¨ï¼Œæ¨å®šç²¾åº¦ãŒä¸ŠãŒã‚‹ã€‚

### æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\xi$ã®è€ƒå¯Ÿ


```python
N=13
#å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®å¹³å‡äºŒä¹—èª¤å·®ï¼ˆMSE)ã®è¡¨ç¤º
result=[]
#diff = DataFrame(columns=['gzai','Training','Test'])
for gzai_i in range(1,1000):

    gzai=gzai_i*0.01

    lss_c, M, Q = lms(x_train, y_train,  N, gzai)

    #å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦yã‚’æ¨å®š
    ys_train = np.polyval(lss_c,x_train)

    #æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦yã‚’æ¨å®š
    ys_test=np.polyval(lss_c,x_test)

    #å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹yã®æ¨å®šã®å¹³å‡äºŒä¹—èª¤å·®
    train_error = mean_squared_error(y_train, ys_train)

    #æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹yã®æ¨å®šã®å¹³å‡äºŒä¹—èª¤å·®
    test_error = mean_squared_error(y_test, ys_test)

    result.append([gzai, train_error, test_error])

diff = DataFrame(data=result, columns=['gzai','Training','Test'])

#å›å¸°ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡ã®çµæœè¡¨ç¤º
diff.plot(x='gzai', y=['Training', 'Test'])

plt.title('Mean Squared Error(sigma = %.3f)' % (sigma))
plt.xlabel('gzai')
plt.ylabel('MSE')
plt.grid(True)
```


    
![png](output_13_0.png)
    



```python
N = 13
gzai=0.32

lss_c, M, Q =lms(x_train, y_train, N, gzai)

xs = np.linspace(np.min(x_train),np.max(x_train),M)
ys = np.polyval(lss_c,xs)

#å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦yã‚’æ¨å®š
ys_train = np.polyval(lss_c,x_train)

# çœŸã®æ›²ç·šã‚’è¡¨ç¤º
linex = np.arange(0, 2.0, 0.01)
liney = np.sin(np.pi*linex) - np.cos(np.pi*linex)
plt.plot(linex, liney, color='green')

plt.scatter(x_train, y_train,label='input')
plt.plot(xs, ys, 'r-', lw=2, label='polyfit')


plt.title('N = %d M= %d  gzai = %f' % (N, M, gzai))
plt.xlabel('x')
plt.ylabel('y')
plt.legend(loc='best',frameon=False)
plt.grid(True)

#å›å¸°ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡ï¼ˆå¹³å‡äºŒä¹—èª¤å·®ï¼‰
from sklearn.metrics import mean_squared_error

#æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦yã‚’æ¨å®š
ys_test=np.polyval(lss_c,x_test)

#å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹yã®æ¨å®šã®å¹³å‡äºŒä¹—èª¤å·®
print('MSE train data: ', mean_squared_error(y_train, ys_train))

#æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹yã®æ¨å®šã®å¹³å‡äºŒä¹—èª¤å·®
print('MSE test data: ', mean_squared_error(y_test, ys_test))
```

    MSE train data:  0.056731303121826726
    MSE test data:  0.2686888238391384
    


    
![png](output_14_1.png)
    


ã“ã‚Œã«ã‚ˆã£ã¦ï¼Œæ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é©åˆ‡ã«é¸æŠã™ã‚‹ã“ã¨ã§ï¼Œå­¦ç¿’åŠ¹æœã¨ãƒ†ã‚¹ãƒˆçµæœã«å½±éŸ¿ãŒã‚ã‚‹ã€‚

## é–¢æ•°2


### è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
ã‚µãƒ³ãƒ—ãƒ«ï¼š50ï¼ˆå­¦ç¿’ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†ã‘ã‚‹ã“ã¨ã§ï¼Œè¦³æ¸¬ç©ºé–“M=25ï¼‰

å…ƒã®é–¢æ•°ï¼š
$$y=sin(x)+cos(x), 0<x<5$$

ãƒã‚¤ã‚ºï¼š å¹³å‡0,æ¨™æº–åå·®0.3ã®æ­£è¦åˆ†å¸ƒ


```python
sample = 50
#ãƒã‚¤ã‚ºæ¨™æº–åå·®
sigma=0.3

# ãƒã‚¤ã‚ºï¼ˆæ­£è¦åˆ†å¸ƒï¼‰
# np.random.normal(å¹³å‡ã€æ¨™æº–åå·®ã€ã‚µãƒ³ãƒ—ãƒ«æ•°)
e= np.random.normal(0, sigma, sample)
#eo= np.random.normal(0, sigma, m)


# ä¿¡å·ç”Ÿæˆ
#np.random.uniform(a, b, ã‚µãƒ³ãƒ—ãƒ«æ•°) aã‹ã‚‰ï½‚ã¾ã§ã®ä¸€æ§˜ä¹±æ•°
x=np.random.uniform(0, 5, sample)
y = np.sin(np.pi*x) - np.cos(np.pi*x)  + e

# ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ç”¨ã¨æ¤œè¨¼ç”¨ã«åˆ†å‰²
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.5, test_size = 0.5, random_state = 0)
```


```python
plt.scatter(x_train, y_train)
plt.xlabel('x_train')
plt.ylabel('y_train')
plt.grid(True)
```


    
![png](output_19_0.png)
    



```python
plt.scatter(x_test, y_test)
plt.xlabel('x_test')
plt.ylabel('y_test')
plt.grid(True)
```


    
![png](output_20_0.png)
    


### è§£ç©ºé–“$N$ã®è€ƒå¯Ÿ


```python
lss_c=[]
M =[]
Q = []
diff =[]
result=[]
for N in range(2,13):
  lss_c, M, Q = lms(x_train, y_train, N, 0.1)
 #å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦yã‚’æ¨å®š
  ys_train= np.polyval(lss_c,x_train)
 #æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦yã‚’æ¨å®š
  ys_test=np.polyval(lss_c,x_test)
 #å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹yã®æ¨å®šå€¤ã®å¹³å‡äºŒä¹—èª¤å·®
  train_error= mean_squared_error(y_train, ys_train)
 #æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹yã®æ¨å®šå€¤ã®å¹³å‡äºŒä¹—èª¤å·®
  test_error= mean_squared_error(y_test, ys_test)
  result.append([train_error, test_error])

diff = DataFrame(data=result, columns=['Training','Test'], index=[n for n in range(2,13)])

diff.plot(title='M= %d gzai= 0.1 Mean Squared Error' % (M))
plt.xlabel('N')
plt.ylabel('MSE')
plt.grid(True)
```


    
![png](output_22_0.png)
    


ã“ã“ã§$\xi=0.1$ã®å ´åˆï¼Œ$N>11$ã«ã™ã‚‹ã¨ï¼Œèª¤å·®ãŒé€†ã«å¤§ãããªã‚‹ã€‚ãã®ç†ç”±ã«ã¤ã„ã¦ï¼Œæ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\xi$ãŒé©åˆ‡ã§ã¯ãªã„ã¨è€ƒãˆã‚‹ãŸã‚ã«ï¼Œä»¥ä¸‹ã®ã‚ˆã†ã«è€ƒå¯Ÿã‚’è¡Œã£ãŸã€‚


### æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\xi$ã®è€ƒå¯Ÿ


```python
#å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®å¹³å‡äºŒä¹—èª¤å·®ï¼ˆMSE)ã®è¡¨ç¤º
N=11
result=[]
#diff = DataFrame(columns=['gzai','Training','Test'])
for gzai_i in range(1,1000):

    gzai=gzai_i*50

    lss_c, M, Q = lms(x_train, y_train,  N, gzai)

    #å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦yã‚’æ¨å®š
    ys_train = np.polyval(lss_c,x_train)

    #æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦yã‚’æ¨å®š
    ys_test=np.polyval(lss_c,x_test)

    #å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹yã®æ¨å®šã®å¹³å‡äºŒä¹—èª¤å·®
    train_error = mean_squared_error(y_train, ys_train)

    #æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹yã®æ¨å®šã®å¹³å‡äºŒä¹—èª¤å·®
    test_error = mean_squared_error(y_test, ys_test)

    result.append([gzai, train_error, test_error])

diff = DataFrame(data=result, columns=['gzai','Training','Test'])

#å›å¸°ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡ã®çµæœè¡¨ç¤º
diff.plot(x='gzai', y=['Training', 'Test'])

plt.title('N = %d M= %d Mean Squared Error(sigma = %.3f)' % (N, M, sigma))
plt.xlabel('gzai')
plt.ylabel('MSE')
plt.grid(True)
```


    
![png](output_25_0.png)
    



```python
N = 11
gzai=0.02

lss_c, M, Q =lms(x_train, y_train, N, gzai)

xs = np.linspace(np.min(x_train),np.max(x_train),M)
ys = np.polyval(lss_c,xs)

#å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦yã‚’æ¨å®š
ys_train = np.polyval(lss_c,x_train)

# çœŸã®æ›²ç·šã‚’è¡¨ç¤º
linex = np.arange(0, 5.0, 0.01)
liney = np.sin(np.pi*linex) - np.cos(np.pi*linex)
plt.plot(linex, liney, color='green')

plt.scatter(x_train, y_train,label='input')
plt.plot(xs, ys, 'r-', lw=2, label='polyfit')


plt.title('N = %d M= %d  gzai = %f' % (N, M, gzai))
plt.xlabel('x')
plt.ylabel('y')
plt.legend(loc='best',frameon=False)
plt.grid(True)

#å›å¸°ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡ï¼ˆå¹³å‡äºŒä¹—èª¤å·®ï¼‰
from sklearn.metrics import mean_squared_error

#æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦yã‚’æ¨å®š
ys_test=np.polyval(lss_c,x_test)

#å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹yã®æ¨å®šã®å¹³å‡äºŒä¹—èª¤å·®
print('MSE train data: ', mean_squared_error(y_train, ys_train))

#æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹yã®æ¨å®šã®å¹³å‡äºŒä¹—èª¤å·®
print('MSE test data: ', mean_squared_error(y_test, ys_test))
```

    MSE train data:  0.3014076921109078
    MSE test data:  0.3351574787240519
    


    
![png](output_26_1.png)
    



```python
N = 11
gzai=10000

lss_c, M, Q =lms(x_train, y_train, N, gzai)

xs = np.linspace(np.min(x_train),np.max(x_train),M)
ys = np.polyval(lss_c,xs)

#å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦yã‚’æ¨å®š
ys_train = np.polyval(lss_c,x_train)

# çœŸã®æ›²ç·šã‚’è¡¨ç¤º
linex = np.arange(0, 5.0, 0.01)
liney = np.sin(np.pi*linex) - np.cos(np.pi*linex)
plt.plot(linex, liney, color='green')

plt.scatter(x_train, y_train,label='input')
plt.plot(xs, ys, 'r-', lw=2, label='polyfit')


plt.title('N = %d M= %d  gzai = %f' % (N, M, gzai))
plt.xlabel('x')
plt.ylabel('y')
plt.legend(loc='best',frameon=False)
plt.grid(True)

#å›å¸°ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡ï¼ˆå¹³å‡äºŒä¹—èª¤å·®ï¼‰
from sklearn.metrics import mean_squared_error

#æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦yã‚’æ¨å®š
ys_test=np.polyval(lss_c,x_test)

#å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹yã®æ¨å®šã®å¹³å‡äºŒä¹—èª¤å·®
print('MSE train data: ', mean_squared_error(y_train, ys_train))

#æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹yã®æ¨å®šã®å¹³å‡äºŒä¹—èª¤å·®
print('MSE test data: ', mean_squared_error(y_test, ys_test))
```

    MSE train data:  0.6603476532318381
    MSE test data:  0.8844738123048232
    


    
![png](output_27_1.png)
    



```python
result=[]
for N in range(10,17):
  lss_c, M, Q = lms(x_train, y_train, N, 10000)
 #å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦yã‚’æ¨å®š
  ys_train= np.polyval(lss_c,x_train)
 #æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦yã‚’æ¨å®š
  ys_test=np.polyval(lss_c,x_test)
 #å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹yã®æ¨å®šå€¤ã®å¹³å‡äºŒä¹—èª¤å·®
  train_error= mean_squared_error(y_train, ys_train)
 #æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹yã®æ¨å®šå€¤ã®å¹³å‡äºŒä¹—èª¤å·®
  test_error= mean_squared_error(y_test, ys_test)
  result.append([train_error, test_error])

diff = DataFrame(data=result, columns=['Training','Test'], index=[n for n in range(10,17)])

diff.plot(title='M= %d gzai= 10000 Mean Squared Error' % (M))
plt.xlabel('N')
plt.ylabel('MSE')
plt.grid(True)
```


    
![png](output_28_0.png)
    


$N=11$ã®å ´åˆã§ï¼Œ$\xi$ã‚’è€ƒå¯Ÿã™ã‚‹ã“ã¨ã§ï¼Œæ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒéå¸¸ã«å°ã•ã„æ™‚(0.1)ï¼Œå­¦ç¿’åŠ¹æœãŒã¨ã¦ã‚‚è‰¯ã„ã“ã¨ãŒåˆ†ã‹ã£ãŸã€‚ã—ã‹ã—ï¼Œã“ã®æ™‚ã«ã¯ãƒ†ã‚¹ãƒˆçµæœã¯æœ€é©ã§ã¯ãªã„ã€‚æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¢—åŠ ã¨ã¨ã‚‚ã«ï¼Œãƒ†ã‚¹ãƒˆçµæœãŒã‚ˆããªã‚‹ãŒï¼Œå­¦ç¿’åŠ¹æœãŒä½ä¸‹ã—ã¦ã„ã‚‹ã€‚


```python
N = 15
gzai=10000

lss_c, M, Q =lms(x_train, y_train, N, gzai)

xs = np.linspace(np.min(x_train),np.max(x_train),M)
ys = np.polyval(lss_c,xs)

#å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦yã‚’æ¨å®š
ys_train = np.polyval(lss_c,x_train)

# çœŸã®æ›²ç·šã‚’è¡¨ç¤º
linex = np.arange(0, 5.0, 0.01)
liney = np.sin(np.pi*linex)- np.cos(np.pi*linex)
plt.plot(linex, liney, color='green')

plt.scatter(x_train, y_train,label='input')
plt.plot(xs, ys, 'r-', lw=2, label='polyfit')


plt.title('N = %d M= %d  gzai = %f' % (N, M, gzai))
plt.xlabel('x')
plt.ylabel('y')
plt.legend(loc='best',frameon=False)
plt.grid(True)

#å›å¸°ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡ï¼ˆå¹³å‡äºŒä¹—èª¤å·®ï¼‰
from sklearn.metrics import mean_squared_error

#æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦yã‚’æ¨å®š
ys_test=np.polyval(lss_c,x_test)

#å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹yã®æ¨å®šã®å¹³å‡äºŒä¹—èª¤å·®
print('MSE train data: ', mean_squared_error(y_train, ys_train))

#æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹yã®æ¨å®šã®å¹³å‡äºŒä¹—èª¤å·®
print('MSE test data: ', mean_squared_error(y_test, ys_test))
```

    MSE train data:  0.42342416627062157
    MSE test data:  0.8058493745194096
    


    
![png](output_30_1.png)
    


ã“ã‚Œã«ã‚ˆã£ã¦ï¼Œæ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¤§ããã™ã‚‹ã¨ï¼Œ$N>11$ã§ã‚‚èª¤å·®ãŒæ™®é€šã«ãªã‚‹ã€‚

### è€ƒå¯Ÿ

ã—ãŸãŒã£ã¦ï¼Œä»¥ä¸‹ã®ã‚ˆã†ãªè€ƒå¯ŸçµæœãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

1. è§£ç©ºé–“$N$ãŒå¤§ãã„ã»ã©ç·šå½¢å›å¸°ã®æ­£å‰‡åŒ–ãƒŸãƒ‹ãƒãƒ ãƒãƒ«ãƒ è§£ãŒã‚ˆããªã‚‹ã€‚ã¾ãŸï¼Œæ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\xi$ ãŒå°ã•ãã™ã‚‹ã¨ï¼Œå­¦ç¿’åŠ¹æœãŒè‰¯ããªã‚‹ã€‚æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\xi$ ãŒå¤§ãããªã‚‹ã¨ï¼Œå­¦ç¿’åŠ¹æœãŒæ‚ªããªã‚‹ãŒï¼Œãƒ†ã‚¹ãƒˆçµæœãŒã‚ˆããªã‚‹ã€‚
2. è§£ç©ºé–“$N$ãŒå¤§ãã„å ´åˆã«é€†ã«åŠ¹æœãŒéå¸¸ã«æ‚ªããªã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã€‚ã¾ãŸï¼Œæ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå‰è¿°ã¨åŒã˜åŠ¹æœãŒã‚ã‚‹ã€‚ãŸã ã—ï¼Œå¤§ããªè§£ç©ºé–“$N$ã‚’åˆã‚ã›ã‚‹ãŸã‚ã«é©åˆ‡ãªéå¸¸ã«å¤§ãã„æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é¸æŠã™ã‚‹ã™ã‚Œã°ï¼Œè‰¯ã„çµæœãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

ã—ã‹ã—ï¼Œä»Šå›ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¯ï¼Œè§£ãé–¢æ•°`lms()`å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æ•°ã‚’è¦³æ¸¬ç©ºé–“$M$ã¨ã—ã¦ã„ã‚‹ã€‚

`M=x_train.shape[0]`

ã“ã‚Œã¯ã‚ãã¾ã§ã‚‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã§ã‚ã‚Šï¼Œä¿¡å·é–¢æ•°ãŒå®Ÿéš›ã«ã¯éç·šå½¢é–¢æ•°ã§ã‚ã‚‹ã€‚é–¢æ•°1ã§ã¯$0<x<2$ã«å¯¾ã—ã¦ï¼Œ$ã‚µãƒ³ãƒ—ãƒ«M=10$ã‚’å–ã£ãŸã€‚é–¢æ•°2ã§ã¯$0<x<5$ã«å¯¾ã—ã¦ï¼Œ$ã‚µãƒ³ãƒ—ãƒ«M=25$ã‚’å–ã£ãŸã€‚
ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æœŸã®å®šç†ã‚’è€ƒãˆã¦ã‚‚ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æœŸãŒåŒã˜ã§ã‚ã‚‹ã€‚

ä¹±æ•°æ•°åˆ—ã®å½±éŸ¿ã‚’è€ƒãˆãšã«ï¼Œã“ã®ã‚ˆã†ãªå•é¡ŒãŒç”Ÿã˜ã‚‹ã®ã¯ï¼ŒåŸºåº•é–¢æ•°ãŒ$\phi_x(k)=x^k$è¨­è¨ˆã•ã‚Œã‚‹ãŸã‚ã«ï¼Œ$x$ã®ç¯„å›²ãŒå¤§ãã„å ´åˆï¼Œè§£ç©ºé–“$N$ãŒå¤§ãã„ã»ã©åŸºåº•é–¢æ•°ãŒæŒ‡æ•°çš„ã«å¢—åŠ ã™ã‚‹ãŸã‚ï¼Œæ¨å®šåŠ¹æœãŒæ‚ªããªã‚Šï¼Œèª¤å·®ãŒéå¸¸ã«å¤§ãããªã‚‹ã€‚

## çµè«–
1. ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã• ğ‘ ã‚’é¸æŠã™ã‚‹æ™‚ã«ï¼Œè¦³æ¸¬ç©ºé–“$M$ã‚’æ¯”ã¹ã¦å¤§ãã„ã»ã©ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ãŒè‰¯ããªã‚‹ãŒï¼Œè¨­è¨ˆã—ãŸåŸºåº•é–¢æ•° $\phi_x(k)$ã‚’è€ƒæ…®ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
2.æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ğœ‰ ã¯éå‰°å­¦ç¿’ã‚’é˜²ããŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹ãŒã€éåº¦ã«å¤§ãã™ãã‚‹ã¨ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ãŒä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€é©åˆ‡ãªå€¤ã‚’é¸æŠã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã‚ã‚‹ã€‚
